# -*- coding: utf-8 -*-

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_eajFZ1ukbmpMwEO0XZQZFlwreZ4ZTf3

<font color=Red><h1><b>
// Make a copy of this notebook if you want to save your work.
</b></h1></font>

<font color=DarkGray><h3><b>
To do this, you can go to *File > Save a Copy in Drive* and it should open a new tab with your copy.
</b></h3></font>

# Deploying Your Object Detection App!

![](https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/object-detection-image.png)

Now that we have our model trained and set up, we can deploy it as a full scale application to the web! While your site is running, you'll be able to use it on your laptop or computer or share it with friends! We'll be using [Streamlit](https://www.streamlit.io/), a library of website objects and methods that allows us to quickly build a site.

# Part 1. Streamlit - Deploying your model to the web
Today we will be using Streamlit, a framework to easily build web applications, to deploy our models to the web so that they can be shared to the web!

Take a moment to look through examples of websites built with Streamlit [here](https://streamlit.io/gallery?category=favorites). As a class, choose your favorite and answer the following **questions:**
* Who is this application for?
* How does the user input data - are these intuitive ways of interacting with the app?
* What does the application do with the data?
* Evaluate the ease of use and look of the application.

Now that we‚Äôve seen what is possible with Streamlit, let‚Äôs try to deploy our **Object Detection model** to the web!
"""

#@title Run this to download data and prepare our environment! { display-mode: "form" }

import matplotlib.pyplot as plt
import os
from PIL import Image
import gdown

import argparse
import numpy as np
from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D
from keras.layers import concatenate, add
from keras.models import Model
import struct
import cv2
from copy import deepcopy

import os
import sys

class HiddenPrints:
    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout

with HiddenPrints():
    # Prepare data
    DATA_ROOT = '/content/data'
    os.makedirs(DATA_ROOT, exist_ok=True)

    !pip -q install streamlit
    !pip -q install pyngrok

    from pyngrok import ngrok
    import streamlit

    # image_url = 'https://drive.google.com/uc?id=12ZpZ5H0kJIkWk6y4ktGfqR5OTKofL7qw'
    # image_path = os.path.join(DATA_ROOT, 'image.jpg')
    # gdown.download(image_url, image_path, True)
    !wget -O /content/data/image.jpg "https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/image.jpg"

    # image2_url = 'https://drive.google.com/uc?id=1_WpFbGEuS2r19UeP6wekbcF0kb-0nH18'
    # image2_path = os.path.join(DATA_ROOT, 'image2.jpg')
    # gdown.download(image2_url, image2_path, True)
    !wget -O /content/data/image2.jpg "https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/image2.jpg"

    # video_url = 'https://drive.google.com/uc?id=1xFGjpzhZVYtNor9hJevvxysGESZJIMDz'
    # video_path = os.path.join(DATA_ROOT, 'video1.mp4')
    # gdown.download(video_url, video_path, True)
    !wget -O /content/data/video1.mp4 "https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/6.mp4"

    # model_url = 'https://drive.google.com/uc?id=19XKJWMKDfDlag2MR8ofjwvxhtr9BxqqN'
    # model_path = os.path.join(DATA_ROOT, 'yolo_weights.h5')
    # gdown.download(model_url, model_path, True)
    !wget -O /content/data/yolo_weights.h5 "https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/yolo.h5"



labels = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck", \
              "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", \
              "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", \
              "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", \
              "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", \
              "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", \
              "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", \
              "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse", \
              "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", \
              "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]

class BoundBox:
    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):
        self.xmin = xmin
        self.ymin = ymin
        self.xmax = xmax
        self.ymax = ymax

        self.objness = objness
        self.classes = classes

        self.label = -1
        self.score = -1

    def get_label(self):
        if self.label == -1:
            self.label = np.argmax(self.classes)

        return self.label

    def get_score(self):
        if self.score == -1:
            self.score = self.classes[self.get_label()]

        return self.score

def _interval_overlap(interval_a, interval_b):
    x1, x2 = interval_a
    x3, x4 = interval_b

    if x3 < x1:
        if x4 < x1:
            return 0
        else:
            return min(x2,x4) - x1
    else:
        if x2 < x3:
             return 0
        else:
            return min(x2,x4) - x3

def _sigmoid(x):
    return 1. / (1. + np.exp(-x))

def bbox_iou(box1, box2):
    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])

    intersect = intersect_w * intersect_h

    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin
    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin

    union = w1*h1 + w2*h2 - intersect

    return float(intersect) / union

def preprocess_input(image_pil, net_h, net_w):
    image = np.asarray(image_pil)
    new_h, new_w, _ = image.shape
    # print("net:", net_h, net_w)
    # print("old:",new_h, new_w)
    # determine the new size of the image
    if (float(net_w)/new_w) < (float(net_h)/new_h):
        new_h = (new_h * net_w)/new_w
        new_w = net_w
    else:
        new_w = (new_w * net_h)/new_h
        new_h = net_h
    new_w = int(new_w)
    new_h = int(new_h)
    # print("new:",int(new_h), int(new_w))
    # resize the image to the new size
    #resized = cv2.resize(image[:,:,::-1]/255., (int(new_w), int(new_h)))
    resized = cv2.resize(image/255., (int(new_w), int(new_h)))

    # embed the image into the standard letter box
    # print("dims:",int((net_h-new_h)//2), int((net_h+new_h)//2), int((net_w-new_w)//2), int((net_w+new_w)//2))
    new_image = np.ones((net_h, net_w, 3)) * 0.5
    new_image[int((net_h-new_h)//2):int((net_h+new_h)//2), int((net_w-new_w)//2):int((net_w+new_w)//2), :] = resized
    new_image = np.expand_dims(new_image, 0)
    # print(new_image.shape)


    return new_image


def decode_netout(netout_, obj_thresh, anchors_, image_h, image_w, net_h, net_w):
    netout_all = deepcopy(netout_)
    boxes_all = []
    for i in range(len(netout_all)):
      netout = netout_all[i][0]
      anchors = anchors_[i]

      grid_h, grid_w = netout.shape[:2]
      nb_box = 3
      netout = netout.reshape((grid_h, grid_w, nb_box, -1))
      nb_class = netout.shape[-1] - 5

      boxes = []

      netout[..., :2]  = _sigmoid(netout[..., :2])
      netout[..., 4:]  = _sigmoid(netout[..., 4:])
      netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]
      netout[..., 5:] *= netout[..., 5:] > obj_thresh

      for i in range(grid_h*grid_w):
          row = i // grid_w
          col = i % grid_w

          for b in range(nb_box):
              # 4th element is objectness score
              objectness = netout[row][col][b][4]
              #objectness = netout[..., :4]
              # last elements are class probabilities
              classes = netout[row][col][b][5:]

              if((classes <= obj_thresh).all()): continue

              # first 4 elements are x, y, w, and h
              x, y, w, h = netout[row][col][b][:4]

              x = (col + x) / grid_w # center position, unit: image width
              y = (row + y) / grid_h # center position, unit: image height
              w = anchors[b][0] * np.exp(w) / net_w # unit: image width
              h = anchors[b][1] * np.exp(h) / net_h # unit: image height

              box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)
              #box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, None, classes)

              boxes.append(box)

      boxes_all += boxes

    # Correct boxes
    boxes_all = correct_yolo_boxes(boxes_all, image_h, image_w, net_h, net_w)

    return boxes_all

def correct_yolo_boxes(boxes_, image_h, image_w, net_h, net_w):
    boxes = deepcopy(boxes_)
    if (float(net_w)/image_w) < (float(net_h)/image_h):
        new_w = net_w
        new_h = (image_h*net_w)/image_w
    else:
        new_h = net_w
        new_w = (image_w*net_h)/image_h

    for i in range(len(boxes)):
        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w
        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h

        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)
    return boxes

def do_nms(boxes_, nms_thresh, obj_thresh):
    boxes = deepcopy(boxes_)
    if len(boxes) > 0:
        num_class = len(boxes[0].classes)
    else:
        return

    for c in range(num_class):
        sorted_indices = np.argsort([-box.classes[c] for box in boxes])

        for i in range(len(sorted_indices)):
            index_i = sorted_indices[i]

            if boxes[index_i].classes[c] == 0: continue

            for j in range(i+1, len(sorted_indices)):
                index_j = sorted_indices[j]

                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
                    boxes[index_j].classes[c] = 0

    new_boxes = []
    for box in boxes:
        label = -1

        for i in range(num_class):
            if box.classes[i] > obj_thresh:
                label = i
                # print("{}: {}, ({}, {})".format(labels[i], box.classes[i]*100, box.xmin, box.ymin))
                box.label = label
                box.score = box.classes[i]
                new_boxes.append(box)

    return new_boxes


from PIL import ImageDraw, ImageFont
import colorsys

def draw_boxes(image_, boxes, labels):
    image = image_.copy()
    image_w, image_h = image.size
    font = ImageFont.truetype(font='/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf',
                    size=np.floor(3e-2 * image_h + 0.5).astype('int32'))
    thickness = (image_w + image_h) // 300

    # Generate colors for drawing bounding boxes.
    hsv_tuples = [(x / len(labels), 1., 1.)
                  for x in range(len(labels))]
    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
    colors = list(
        map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))
    np.random.seed(10101)  # Fixed seed for consistent colors across runs.
    np.random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.
    np.random.seed(None)  # Reset seed to default.

    for i, box in reversed(list(enumerate(boxes))):
        c = box.get_label()
        predicted_class = labels[c]
        score = box.get_score()
        top, left, bottom, right = box.ymin, box.xmin, box.ymax, box.xmax

        label = '{} {:.2f}'.format(predicted_class, score)
        draw = ImageDraw.Draw(image)
        label_size = draw.textbbox((0,0), label, font)
        label_size = (label_size[2], label_size[3])

        top = max(0, np.floor(top + 0.5).astype('int32'))
        left = max(0, np.floor(left + 0.5).astype('int32'))
        bottom = min(image_h, np.floor(bottom + 0.5).astype('int32'))
        right = min(image_w, np.floor(right + 0.5).astype('int32'))
        print(label, (left, top), (right, bottom))

        # Check if the box is valid BEFORE drawing
        if right > left and bottom > top:
            if top - label_size[1] >= 0:
                text_origin = np.array([left, top - label_size[1]])
            else:
                text_origin = np.array([left, top + 1])

            # Draw the bounding box
            for i in range(thickness):
                # Additional check to ensure valid rectangle coordinates
                if (right - i) > (left + i) and (bottom - i) > (top + i):
                    draw.rectangle(
                        [left + i, top + i, right - i, bottom - i],
                        outline=colors[c])

            # Draw the label background and text
            draw.rectangle(
                [tuple(text_origin), tuple(text_origin + label_size)],
                fill=colors[c])
            draw.text(text_origin, label, fill=(0, 0, 0), font=font)
        else:
            print(f"Skipping invalid box: {(left, top, right, bottom)}")

        del draw

    return image

def launch_website():
  try:
    # Kill any existing ngrok tunnels
    ngrok.kill()

    tunnel = ngrok.connect()

    print("Click this link to try your web app:")
    print(tunnel.public_url)

    !streamlit run --server.port 80 app.py # Connect to the URL through Port 80

  except KeyboardInterrupt:
    ngrok.kill()

"""First, let's make sure that we are using our GPU! <br> This block of code should output `Found GPU at: /device:GPU:0`. <br> If not, go to your `Runtime` tab, and `Change Runtime`.
- Select a T4 GPU or TPU under the Hardware Accelerator section!
"""

import tensorflow as tf

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print('No GPU Found! D:')
else:
  print('Found GPU at: {}'.format(device_name))

"""When dealing with Streamlit and web development, we begin to work with **file management**. In Python web development, such as Flask and Streamlit, we use a central Python file to launch our app. In this case, we will use `app.py`.
```
app.py
```

Using Streamlit, we can build a simple webapp like so:

```
%%writefile app.py
```
The `%%writefile` command writes to a file, either overwriting what's already there or creating it if it doesn't already exist. Everything that follows the rest of this block of code will be written to `app.py`.

In this case, we use this command to create our `app.py` file, which you can check by clicking on the folder on the left sidebar.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# st.title('Hello World!')

"""To initiate our `app.py`, we employ both Streamlit and ngrok in tandem. ngrok serves as a hosting service that enables us to establish secure tunnels to our localhost, making our local server accessible over the internet. Streamlit, on the other hand, is a powerful library designed to turn Python scripts into interactive web applications easily. By integrating Streamlit with ngrok, we create a bridge that connects our Python application to the web, allowing external access.

<br> To get access to our ngrok server, we need to sign up on their website and get a unique **authentication token**.

<font color=SlateGrey><h2><b>
Use [these](https://drive.google.com/file/d/12zwuOuKh91VSHIHS-6S4ADF4HLC2wKJq/view?usp=sharing) instructions to create a ngrok account and get your authtoken!
</b></h2></font>

<font color=DarkGray><h3><b>
Paste your authtoken below next to `!ngrok authtoken`!
</b></h3></font>
"""

!ngrok authtoken 30ATeAiEBVtZZe9ktBofhd01Czp_6C6GJzfueY2dXF78mCrkr

"""Now, we can launch our website through the `launch_website()` function, which connects our ngrok token by building a 'tunnel' to our Streamlit code.

Run the following code now to use our function. Click on the link, and hit the `Visit Site` button to view your site!
"""

launch_website()

"""Now the question remains, how do we connect our computer vision models to our website?

# Part 2. Connecting our model to our website

Recall that earlier, we wrote the function `detect_image()` with the arguments `image_pil, obj_thresh=0.4, nms_thresh=0.45, darknet=darknet, net_h=416, net_w=416, anchors=anchors, labels=labels`. <br><br> `anchors` and `labels` are variables that had been defined previously in the last notebook. Keep in mind that our server only has access to the files that we write to. In this case, we only have `app.py`! <br><br> This may have been what our `detect_image` function looked like:
```python
def detect_image(image_pil, obj_thresh = 0.4, nms_thresh = 0.45, darknet=darknet, net_h=416, net_w=416, anchors=anchors, labels=labels):

  # Preprocessing
  image_w, image_h = image_pil.size
  new_image = preprocess_input(image_pil, net_h, net_w)

  # DarkNet
  yolo_outputs = darknet.predict(new_image)

  # Decode the output of the network
  boxes = decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)

  # Suppress non-maximal boxes
  boxes = do_nms(boxes, nms_thresh, obj_thresh)

  # Draw bounding boxes on the image using labels
  image_detect = draw_boxes(image_pil, boxes, labels)

  return image_detect
```

However, notice how many variables and imports we needed. <br><br>For `yolo_outputs = darknet.predict(new_image)`, we needed to import the entire darknet model (which our current website doesn't have access to!), the anchors, labels, etc. <br><br>In order to get this all working in our `app.py`, we would have to paste all of these variables in, including ***ALL*** of the hidden functions we used in our last notebook! For the imports alone, it would look something like this:
"""

# Commented out IPython magic to ensure Python compatibility.
# #@title Click the dropdown to display all the code you'd be pasting in
# %%writefile app.py
# import tensorflow as tf
# import matplotlib.pyplot as plt
# import os
# from PIL import Image
# import argparse
# import numpy as np
# from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D
# from keras.layers import concatenate, add
# from keras.models import Model
# import struct
# import cv2
# from copy import deepcopy
# 
# anchors = [[[116,90], [156,198], [373,326]], [[30,61], [62,45], [59,119]], [[10,13], [16,30], [33,23]]]
# 
# DATA_ROOT = '/content/data'
# 
# model_path = os.path.join(DATA_ROOT, 'yolo_weights.h5')
# 
# darknet = tf.keras.models.load_model(model_path, compile=False)
# 
# labels = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck", \
#               "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", \
#               "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", \
#               "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", \
#               "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", \
#               "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", \
#               "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", \
#               "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse", \
#               "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", \
#               "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]
# 
# class BoundBox:
#     def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):
#         self.xmin = xmin
#         self.ymin = ymin
#         self.xmax = xmax
#         self.ymax = ymax
# 
#         self.objness = objness
#         self.classes = classes
# 
#         self.label = -1
#         self.score = -1
# 
#     def get_label(self):
#         if self.label == -1:
#             self.label = np.argmax(self.classes)
# 
#         return self.label
# 
#     def get_score(self):
#         if self.score == -1:
#             self.score = self.classes[self.get_label()]
# 
#         return self.score
# 
# def _interval_overlap(interval_a, interval_b):
#     x1, x2 = interval_a
#     x3, x4 = interval_b
# 
#     if x3 < x1:
#         if x4 < x1:
#             return 0
#         else:
#             return min(x2,x4) - x1
#     else:
#         if x2 < x3:
#              return 0
#         else:
#             return min(x2,x4) - x3
# 
# def _sigmoid(x):
#     return 1. / (1. + np.exp(-x))
# 
# def bbox_iou(box1, box2):
#     intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
#     intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])
# 
#     intersect = intersect_w * intersect_h
# 
#     w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin
#     w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin
# 
#     union = w1*h1 + w2*h2 - intersect
# 
#     return float(intersect) / union
# 
# def preprocess_input(image_pil, net_h, net_w):
#     image = np.asarray(image_pil)
#     # Remove the alpha channel if it exists
#     if image.shape[2] == 4:
#         image = image[:, :, :3]
# 
#     new_h, new_w, _ = image.shape
#     if (float(net_w)/new_w) < (float(net_h)/new_h):
#         new_h = (new_h * net_w)/new_w
#         new_w = net_w
#     else:
#         new_w = (new_w * net_h)/new_h
#         new_h = net_h
# 
#     new_w = int(new_w)
#     new_h = int(new_h)
# 
#     resized = cv2.resize(image/255., (int(new_w), int(new_h)))
# 
#     new_image = np.ones((net_h, net_w, 3)) * 0.5
#     new_image[int((net_h-new_h)//2):int((net_h+new_h)//2), int((net_w-new_w)//2):int((net_w+new_w)//2), :] = resized
#     new_image = np.expand_dims(new_image, 0)
# 
#     return new_image
# 
# 
# def decode_netout(netout_, obj_thresh, anchors_, image_h, image_w, net_h, net_w):
#     netout_all = deepcopy(netout_)
#     boxes_all = []
#     for i in range(len(netout_all)):
#       netout = netout_all[i][0]
#       anchors = anchors_[i]
# 
#       grid_h, grid_w = netout.shape[:2]
#       nb_box = 3
#       netout = netout.reshape((grid_h, grid_w, nb_box, -1))
#       nb_class = netout.shape[-1] - 5
# 
#       boxes = []
# 
#       netout[..., :2]  = _sigmoid(netout[..., :2])
#       netout[..., 4:]  = _sigmoid(netout[..., 4:])
#       netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]
#       netout[..., 5:] *= netout[..., 5:] > obj_thresh
# 
#       for i in range(grid_h*grid_w):
#           row = i // grid_w
#           col = i % grid_w
# 
#           for b in range(nb_box):
#               # 4th element is objectness score
#               objectness = netout[row][col][b][4]
#               #objectness = netout[..., :4]
#               # last elements are class probabilities
#               classes = netout[row][col][b][5:]
# 
#               if((classes <= obj_thresh).all()): continue
# 
#               # first 4 elements are x, y, w, and h
#               x, y, w, h = netout[row][col][b][:4]
# 
#               x = (col + x) / grid_w # center position, unit: image width
#               y = (row + y) / grid_h # center position, unit: image height
#               w = anchors[b][0] * np.exp(w) / net_w # unit: image width
#               h = anchors[b][1] * np.exp(h) / net_h # unit: image height
# 
#               box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)
#               #box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, None, classes)
# 
#               boxes.append(box)
# 
#       boxes_all += boxes
# 
#     # Correct boxes
#     boxes_all = correct_yolo_boxes(boxes_all, image_h, image_w, net_h, net_w)
# 
#     return boxes_all
# 
# def correct_yolo_boxes(boxes_, image_h, image_w, net_h, net_w):
#     boxes = deepcopy(boxes_)
#     if (float(net_w)/image_w) < (float(net_h)/image_h):
#         new_w = net_w
#         new_h = (image_h*net_w)/image_w
#     else:
#         new_h = net_w
#         new_w = (image_w*net_h)/image_h
# 
#     for i in range(len(boxes)):
#         x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w
#         y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h
# 
#         boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
#         boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
#         boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
#         boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)
#     return boxes
# 
# def do_nms(boxes_, nms_thresh, obj_thresh):
#     boxes = deepcopy(boxes_)
#     if len(boxes) > 0:
#         num_class = len(boxes[0].classes)
#     else:
#         return
# 
#     for c in range(num_class):
#         sorted_indices = np.argsort([-box.classes[c] for box in boxes])
# 
#         for i in range(len(sorted_indices)):
#             index_i = sorted_indices[i]
# 
#             if boxes[index_i].classes[c] == 0: continue
# 
#             for j in range(i+1, len(sorted_indices)):
#                 index_j = sorted_indices[j]
# 
#                 if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
#                     boxes[index_j].classes[c] = 0
# 
#     new_boxes = []
#     for box in boxes:
#         label = -1
# 
#         for i in range(num_class):
#             if box.classes[i] > obj_thresh:
#                 label = i
#                 # print("{}: {}, ({}, {})".format(labels[i], box.classes[i]*100, box.xmin, box.ymin))
#                 box.label = label
#                 box.score = box.classes[i]
#                 new_boxes.append(box)
# 
#     return new_boxes
# 
# 
# from PIL import ImageDraw, ImageFont
# import colorsys
# 
# def draw_boxes(image_, boxes, labels):
#     image = image_.copy()
#     image_w, image_h = image.size
#     font = ImageFont.truetype(font='/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf',
#                     size=np.floor(3e-2 * image_h + 0.5).astype('int32'))
#     thickness = (image_w + image_h) // 300
# 
#     # Generate colors for drawing bounding boxes.
#     hsv_tuples = [(x / len(labels), 1., 1.)
#                   for x in range(len(labels))]
#     colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
#     colors = list(
#         map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))
#     np.random.seed(10101)  # Fixed seed for consistent colors across runs.
#     np.random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.
#     np.random.seed(None)  # Reset seed to default.
# 
#     for i, box in reversed(list(enumerate(boxes))):
#         c = box.get_label()
#         predicted_class = labels[c]
#         score = box.get_score()
#         top, left, bottom, right = box.ymin, box.xmin, box.ymax, box.xmax
# 
#         label = '{} {:.2f}'.format(predicted_class, score)
#         draw = ImageDraw.Draw(image)
#         label_size = draw.textbbox((0,0), label, font)
#         label_size = (label_size[2], label_size[3])
# 
#         top = max(0, np.floor(top + 0.5).astype('int32'))
#         left = max(0, np.floor(left + 0.5).astype('int32'))
#         bottom = min(image_h, np.floor(bottom + 0.5).astype('int32'))
#         right = min(image_w, np.floor(right + 0.5).astype('int32'))
#         print(label, (left, top), (right, bottom))
# 
#         # Check if the box is valid BEFORE drawing
#         if right > left and bottom > top:
#             if top - label_size[1] >= 0:
#                 text_origin = np.array([left, top - label_size[1]])
#             else:
#                 text_origin = np.array([left, top + 1])
# 
#             # Draw the bounding box
#             for i in range(thickness):
#                 # Additional check to ensure valid rectangle coordinates
#                 if (right - i) > (left + i) and (bottom - i) > (top + i):
#                     draw.rectangle(
#                         [left + i, top + i, right - i, bottom - i],
#                         outline=colors[c])
# 
#             # Draw the label background and text
#             draw.rectangle(
#                 [tuple(text_origin), tuple(text_origin + label_size)],
#                 fill=colors[c])
#             draw.text(text_origin, label, fill=(0, 0, 0), font=font)
#         else:
#             print(f"Skipping invalid box: {(left, top, right, bottom)}")
# 
#         del draw
# 
#     return image
# 
# def detect_image(image_pil, obj_thresh = 0.4, nms_thresh = 0.45, darknet=darknet, net_h=416, net_w=416, anchors=anchors, labels=labels):
# 
#   # Preprocessing
#   image_w, image_h = image_pil.size
#   new_image = preprocess_input(image_pil, net_h, net_w)
# 
#   # DarkNet
#   yolo_outputs = darknet.predict(new_image)
# 
#   # Decode the output of the network
#   boxes = decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)
# 
#   # Suppress non-maximal boxes
#   boxes = do_nms(boxes, nms_thresh, obj_thresh)
# 
#   # Draw bounding boxes on the image using labels
#   image_detect = draw_boxes(image_pil, boxes, labels)
# 
#   return image_detect
# 
# # Streamlit app
# st.title('Object Detection')

"""with our tiny
```python
import streamlit as st

st.title('Hello World!')
```
at the end!

Instead, we can write all our code to another file called **`utils.py`**.<br> This way, we can instead add this to the top of our **`app.py`** file.
```python
from utils.py import *
```
The `import *` imports all variables and functions from our **`utils.py`** file. You can also use
```python
from utils.py import anchors, model_path
```
if you wanted to import only **`anchors`** and **`model_path`**, but there are a lot of functions and variables to list we want to use. Now, our code might look like this for **`app.py`**:
```python
from utils.py import *
import streamlit as st

st.title('Hello World!')
```

#### Exercise: Write to a **`utils.py`** file!
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile utils.py
# import tensorflow as tf
# import matplotlib.pyplot as plt
# import os
# from PIL import Image
# import argparse
# import numpy as np
# from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D
# from keras.layers import concatenate, add
# from keras.models import Model
# import struct
# import cv2
# from copy import deepcopy
# from PIL import ImageDraw, ImageFont
# import colorsys
# 
# 
# anchors = [[[116,90], [156,198], [373,326]], [[30,61], [62,45], [59,119]], [[10,13], [16,30], [33,23]]]
# 
# DATA_ROOT = '/content/data'
# 
# model_path = os.path.join(DATA_ROOT, 'yolo_weights.h5')
# 
# darknet = tf.keras.models.load_model(model_path, compile=False)
# 
# labels = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck", \
#               "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", \
#               "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", \
#               "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", \
#               "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", \
#               "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", \
#               "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", \
#               "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse", \
#               "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", \
#               "book", "clock", "vase", "scissors", "teddy bear", "hair dryer", "toothbrush"]
# 
# class BoundBox:
#     def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):
#         self.xmin = xmin
#         self.ymin = ymin
#         self.xmax = xmax
#         self.ymax = ymax
# 
#         self.objness = objness
#         self.classes = classes
# 
#         self.label = -1
#         self.score = -1
# 
#     def get_label(self):
#         if self.label == -1:
#             self.label = np.argmax(self.classes)
# 
#         return self.label
# 
#     def get_score(self):
#         if self.score == -1:
#             self.score = self.classes[self.get_label()]
# 
#         return self.score
# 
# def _interval_overlap(interval_a, interval_b):
#     x1, x2 = interval_a
#     x3, x4 = interval_b
# 
#     if x3 < x1:
#         if x4 < x1:
#             return 0
#         else:
#             return min(x2,x4) - x1
#     else:
#         if x2 < x3:
#              return 0
#         else:
#             return min(x2,x4) - x3
# 
# def _sigmoid(x):
#     return 1. / (1. + np.exp(-x))
# 
# def bbox_iou(box1, box2):
#     intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
#     intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])
# 
#     intersect = intersect_w * intersect_h
# 
#     w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin
#     w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin
# 
#     union = w1*h1 + w2*h2 - intersect
# 
#     return float(intersect) / union
# 
# def preprocess_input(image_pil, net_h, net_w):
#     image = np.asarray(image_pil)
#     # Remove the alpha channel if it exists
#     if image.shape[2] == 4:
#         image = image[:, :, :3]
# 
#     new_h, new_w, _ = image.shape
#     if (float(net_w)/new_w) < (float(net_h)/new_h):
#         new_h = (new_h * net_w)/new_w
#         new_w = net_w
#     else:
#         new_w = (new_w * net_h)/new_h
#         new_h = net_h
# 
#     new_w = int(new_w)
#     new_h = int(new_h)
# 
#     resized = cv2.resize(image/255., (int(new_w), int(new_h)))
# 
#     new_image = np.ones((net_h, net_w, 3)) * 0.5
#     new_image[int((net_h-new_h)//2):int((net_h+new_h)//2), int((net_w-new_w)//2):int((net_w+new_w)//2), :] = resized
#     new_image = np.expand_dims(new_image, 0)
# 
#     return new_image
# 
# 
# def decode_netout(netout_, obj_thresh, anchors_, image_h, image_w, net_h, net_w):
#     netout_all = deepcopy(netout_)
#     boxes_all = []
#     for i in range(len(netout_all)):
#       netout = netout_all[i][0]
#       anchors = anchors_[i]
# 
#       grid_h, grid_w = netout.shape[:2]
#       nb_box = 3
#       netout = netout.reshape((grid_h, grid_w, nb_box, -1))
#       nb_class = netout.shape[-1] - 5
# 
#       boxes = []
# 
#       netout[..., :2]  = _sigmoid(netout[..., :2])
#       netout[..., 4:]  = _sigmoid(netout[..., 4:])
#       netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]
#       netout[..., 5:] *= netout[..., 5:] > obj_thresh
# 
#       for i in range(grid_h*grid_w):
#           row = i // grid_w
#           col = i % grid_w
# 
#           for b in range(nb_box):
#               # 4th element is objectness score
#               objectness = netout[row][col][b][4]
#               #objectness = netout[..., :4]
#               # last elements are class probabilities
#               classes = netout[row][col][b][5:]
# 
#               if((classes <= obj_thresh).all()): continue
# 
#               # first 4 elements are x, y, w, and h
#               x, y, w, h = netout[row][col][b][:4]
# 
#               x = (col + x) / grid_w # center position, unit: image width
#               y = (row + y) / grid_h # center position, unit: image height
#               w = anchors[b][0] * np.exp(w) / net_w # unit: image width
#               h = anchors[b][1] * np.exp(h) / net_h # unit: image height
# 
#               box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)
#               #box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, None, classes)
# 
#               boxes.append(box)
# 
#       boxes_all += boxes
# 
#     # Correct boxes
#     boxes_all = correct_yolo_boxes(boxes_all, image_h, image_w, net_h, net_w)
# 
#     return boxes_all
# 
# def correct_yolo_boxes(boxes_, image_h, image_w, net_h, net_w):
#     boxes = deepcopy(boxes_)
#     if (float(net_w)/image_w) < (float(net_h)/image_h):
#         new_w = net_w
#         new_h = (image_h*net_w)/image_w
#     else:
#         new_h = net_w
#         new_w = (image_w*net_h)/image_h
# 
#     for i in range(len(boxes)):
#         x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w
#         y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h
# 
#         boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
#         boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
#         boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
#         boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)
#     return boxes
# 
# def do_nms(boxes_, nms_thresh, obj_thresh):
#     boxes = deepcopy(boxes_)
#     if len(boxes) > 0:
#         num_class = len(boxes[0].classes)
#     else:
#         return [] # Return an empty list if there are no boxes
# 
#     for c in range(num_class):
#         sorted_indices = np.argsort([-box.classes[c] for box in boxes])
# 
#         for i in range(len(sorted_indices)):
#             index_i = sorted_indices[i]
# 
#             if boxes[index_i].classes[c] == 0: continue
# 
#             for j in range(i+1, len(sorted_indices)):
#                 index_j = sorted_indices[j]
# 
#                 if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
#                     boxes[index_j].classes[c] = 0
# 
#     new_boxes = []
#     for box in boxes:
#         label = -1
# 
#         for i in range(num_class):
#             if box.classes[i] > obj_thresh:
#                 label = i
#                 # print("{}: {}, ({}, {})".format(labels[i], box.classes[i]*100, box.xmin, box.ymin))
#                 box.label = label
#                 box.score = box.classes[i]
#                 new_boxes.append(box)
# 
#     return new_boxes
# 
# 
# def draw_boxes(image_, boxes, labels):
#     image = image_.copy()
#     image_w, image_h = image.size
#     font = ImageFont.truetype(font='/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf',
#                     size=np.floor(3e-2 * image_h + 0.5).astype('int32'))
#     thickness = (image_w + image_h) // 300
# 
#     # Generate colors for drawing bounding boxes.
#     hsv_tuples = [(x / len(labels), 1., 1.)
#                   for x in range(len(labels))]
#     colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
#     colors = list(
#         map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))
#     np.random.seed(10101)  # Fixed seed for consistent colors across runs.
#     np.random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.
#     np.random.seed(None)  # Reset seed to default.
# 
#     for i, box in reversed(list(enumerate(boxes))):
#         c = box.get_label()
#         predicted_class = labels[c]
#         score = box.get_score()
#         top, left, bottom, right = box.ymin, box.xmin, box.ymax, box.xmax
# 
#         label = '{} {:.2f}'.format(predicted_class, score)
#         draw = ImageDraw.Draw(image)
#         label_size = draw.textbbox((0,0), label, font)
#         label_size = (label_size[2], label_size[3])
# 
# 
#         top = max(0, np.floor(top + 0.5).astype('int32'))
#         left = max(0, np.floor(left + 0.5).astype('int32'))
#         bottom = min(image_h, np.floor(bottom + 0.5).astype('int32'))
#         right = min(image_w, np.floor(right + 0.5).astype('int32'))
#         print(label, (left, top), (right, bottom))
# 
#         # Check if the box is valid BEFORE drawing
#         if right > left and bottom > top:
#             if top - label_size[1] >= 0:
#                 text_origin = np.array([left, top - label_size[1]])
#             else:
#                 text_origin = np.array([left, top + 1])
# 
#             # Draw the bounding box
#             for i in range(thickness):
#                 # Additional check to ensure valid rectangle coordinates
#                 if (right - i) > (left + i) and (bottom - i) > (top + i):
#                     draw.rectangle(
#                         [left + i, top + i, right - i, bottom - i],
#                         outline=colors[c])
# 
#             # Draw the label background and text
#             draw.rectangle(
#                 [tuple(text_origin), tuple(text_origin + label_size)],
#                 fill=colors[c])
#             draw.text(text_origin, label, fill=(0, 0, 0), font=font)
#         else:
#             print(f"Skipping invalid box: {(left, top, right, bottom)}")
# 
#         del draw
# 
#     return image
# 
# 
# def detect_image(image_pil, obj_thresh = 0.4, nms_thresh = 0.45, darknet=darknet, net_h=416, net_w=416, anchors=anchors, labels=labels):
# 
#   # Preprocessing
#   image_w, image_h = image_pil.size
#   new_image = preprocess_input(image_pil, net_h, net_w)
# 
#   # DarkNet
#   yolo_outputs = darknet.predict(new_image)
# 
#   # Decode the output of the network
#   boxes = decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)
# 
#   # Suppress non-maximal boxes
#   boxes = do_nms(boxes, nms_thresh, obj_thresh)
# 
#   # Draw bounding boxes on the image using labels
#   image_detect = draw_boxes(image_pil, boxes, labels)
# 
#   return image_detect

"""Now, we want to create a Streamlit object for our site to use to upload images! You can do so by using the following:
```python
uploaded_file = st.file_uploader("Prompt", type=["jpg", "png"])
```
This will display a button with the "Prompt" string, and allows the users to input files of type **`jpg, png`**, which are common image file types.

# Part 3. Putting it all together

Now, let's rewrite our **`app.py`** file!

*Tip*: When we add in the `%%writefile` magic command, Colab removes all syntax highlighting, which makes the code a lot harder to read! If you'd like, you can comment out that `%%writefile` line by adding a `#` at the beginning to restore the syntax highlighting as you edit. Make sure you remove that `#` before you run this, so that your `app.py` file is actually overwritten!
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from utils import detect_image  # You must define this
# from PIL import Image
# import numpy as np
# import cv2
# import tempfile
# import time
# 
# st.title("Object Detection App")
# 
# # Upload image or video
# uploaded_image = st.file_uploader("Upload an image...", type=["jpg", "jpeg", "png"])
# uploaded_video = st.file_uploader("Upload a video...", type=["mp4", "webm"])
# 
# # ----- Image Mode -----
# if uploaded_image is not None:
#     image = Image.open(uploaded_image)
#     st.image(image, caption="Uploaded Image", use_container_width=True)
# 
#     if st.button("üîç Detect Objects in Image"):
#         detected = detect_image(image)
#         st.image(detected, caption="Detected Objects", use_container_width=True)
# 
# # ----- Video Mode -----
# elif uploaded_video is not None:
#     st.video(uploaded_video)
# 
#     if st.button("Detect Objects in Video"):
#         st.info("Processing video, please wait...")
# 
#         # Save uploaded video to a temp file
#         with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as temp_in:
#             temp_in.write(uploaded_video.read())
#             input_video_path = temp_in.name
# 
#         # OpenCV video read
#         cap = cv2.VideoCapture(input_video_path)
#         if not cap.isOpened():
#             st.error("Failed to open video.")
#         else:
#             # Get video properties
#             fps = cap.get(cv2.CAP_PROP_FPS)
#             width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
#             height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
#             total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
# 
#             # Output video temp file (WEBM)
#             with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as temp_out:
#                 output_video_path = temp_out.name
# 
#             fourcc = cv2.VideoWriter_fourcc(*"VP80")  # VP8 codec for .webm
#             out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
# 
#             # Progress
#             frame_num = 0
#             progress = st.progress(0.0)
# 
#             while True:
#                 ret, frame = cap.read()
#                 if not ret:
#                     break
# 
#                 # Convert to PIL
#                 frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
#                 detected_pil = detect_image(frame_pil)
#                 detected_cv2 = cv2.cvtColor(np.array(detected_pil), cv2.COLOR_RGB2BGR)
# 
#                 out.write(detected_cv2)
#                 frame_num += 1
#                 progress.progress(min(frame_num / total_frames, 1.0))
# 
#             cap.release()
#             out.release()
#             time.sleep(1)  # ensure write complete
# 
#             st.success("Done! Showing result:")
#             st.video(output_video_path)
#

"""Here is a basic toolbox to work with, but look things up to explore!
```python
#Text
st.write("text")

#Title
st.title('title')

#Header
st.header('header')

#Slider
value = st.slider('variable')

#Table
st.table(dataframe) # Replace with your own Pandas dataframe variable

#Matplotlib Figure
st.pyplot(fig) # Replace with your own Matplotlib figure variable

#Image
st.image(image, caption='Image Caption') # Replace with your own image variable

#Button
pressed = st.button('Button Name')

#Checkbox
checked = st.checkbox('Checkbox Name')

#File Input
uploaded_file = st.file_uploader("Upload File")

if uploaded_file is not None:
    pass #Do something here!
```

### Finally, let's launch our website again :)
"""

launch_website()

"""Try testing different images for your object detection app by searching for any images on Google! Here's one below for reference (right-click and hit Save As... and upload the image to your app to test!):

![](https://www.stockvault.net/data/2016/03/12/187548/preview16.jpg)
"""
